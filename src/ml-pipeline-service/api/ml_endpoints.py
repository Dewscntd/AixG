"""
ML Service API Endpoints for Multi-Tier Model Access
Provides direct access to computer vision and ML capabilities
"""

import asyncio
import logging
from typing import Dict, Any, List, Optional, Union
import base64
import io
from PIL import Image
import numpy as np

from fastapi import APIRouter, HTTPException, UploadFile, File, Form, Depends
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field

from ..infrastructure.integrated_ml_service import ml_service
from ..infrastructure.model_registry import ModelCategory, ModelType, model_registry
from ..infrastructure.model_tier_router import RoutingStrategy

logger = logging.getLogger(__name__)

# Pydantic models for API requests/responses

class MLRequest(BaseModel):
    """Base request for ML operations"""
    preferred_tier: Optional[str] = "auto"
    preferred_model: Optional[str] = None
    confidence_threshold: Optional[float] = 0.5
    max_results: Optional[int] = 10

class ObjectDetectionRequest(MLRequest):
    """Request for object detection"""
    image_base64: Optional[str] = None
    detect_classes: Optional[List[str]] = None

class ImageClassificationRequest(MLRequest):
    """Request for image classification"""
    image_base64: Optional[str] = None
    top_k: Optional[int] = 5

class PoseEstimationRequest(MLRequest):
    """Request for pose estimation"""
    image_base64: Optional[str] = None
    keypoint_threshold: Optional[float] = 0.3

class ActionRecognitionRequest(MLRequest):
    """Request for action recognition"""
    video_frames_base64: List[str] = Field(..., description="Base64 encoded video frames")
    sequence_length: Optional[int] = 16

class TextAnalysisRequest(MLRequest):
    """Request for text analysis with LLM"""
    text: str = Field(..., description="Text to analyze")
    prompt: Optional[str] = None
    max_tokens: Optional[int] = 1000

class EmbeddingRequest(MLRequest):
    """Request for generating embeddings"""
    texts: List[str] = Field(..., description="Texts to embed")

class MLResponse(BaseModel):\n    """Base response for ML operations"""\n    success: bool\n    model_used: str\n    tier_used: str\n    response_time_ms: float\n    cost_estimate: float\n    error: Optional[str] = None\n    fallback_used: bool = False\n\nclass DetectionResult(BaseModel):\n    """Object detection result"""\n    class_name: str\n    confidence: float\n    bbox: Dict[str, float]  # x, y, width, height\n\nclass ClassificationResult(BaseModel):\n    """Classification result"""\n    class_name: str\n    confidence: float\n    class_id: Optional[int] = None\n\nclass KeyPoint(BaseModel):\n    """Pose keypoint"""\n    x: float\n    y: float\n    confidence: float\n\nclass PoseResult(BaseModel):\n    """Pose estimation result"""\n    keypoints: List[KeyPoint]\n    pose_confidence: float\n\nclass ActionResult(BaseModel):\n    """Action recognition result"""\n    action: str\n    confidence: float\n    timestamp: Optional[float] = None\n\nclass ObjectDetectionResponse(MLResponse):\n    """Object detection response"""\n    detections: List[DetectionResult] = []\n\nclass ImageClassificationResponse(MLResponse):\n    """Image classification response"""\n    classifications: List[ClassificationResult] = []\n\nclass PoseEstimationResponse(MLResponse):\n    """Pose estimation response"""\n    poses: List[PoseResult] = []\n\nclass ActionRecognitionResponse(MLResponse):\n    """Action recognition response"""\n    actions: List[ActionResult] = []\n\nclass TextAnalysisResponse(MLResponse):\n    """Text analysis response"""\n    analysis: str\n    tokens_used: Optional[int] = None\n\nclass EmbeddingResponse(MLResponse):\n    """Embedding response"""\n    embeddings: List[List[float]] = []\n    dimensions: int\n\nclass ModelInfo(BaseModel):\n    """Model information"""\n    name: str\n    version: str\n    provider: str\n    category: str\n    tier: str\n    description: str\n    loaded: bool\n    within_rate_limits: bool\n\nclass ServiceStatus(BaseModel):\n    """ML service status"""\n    total_models: int\n    loaded_models: int\n    models_by_tier: Dict[str, int]\n    models_by_category: Dict[str, int]\n    routing_strategy: str\n    gpu_available: bool\n    cache_enabled: bool\n\n# Create router\nrouter = APIRouter(prefix="/ml", tags=["ML Service"])\n\n# Helper functions\n\ndef decode_base64_image(base64_str: str) -> Image.Image:\n    """Decode base64 string to PIL Image"""\n    try:\n        # Remove data URL prefix if present\n        if base64_str.startswith('data:image'):\n            base64_str = base64_str.split(',')[1]\n        \n        image_data = base64.b64decode(base64_str)\n        image = Image.open(io.BytesIO(image_data))\n        return image\n    except Exception as e:\n        raise HTTPException(status_code=400, detail=f"Invalid base64 image: {str(e)}")\n\ndef get_model_tier(tier_str: str) -> Optional[ModelType]:\n    """Convert string to ModelType"""\n    if tier_str == "auto":\n        return None\n    try:\n        return ModelType(tier_str.lower())\n    except ValueError:\n        return None\n\ndef format_ml_response(result, response_class) -> Union[MLResponse, Dict[str, Any]]:\n    """Format ML service result to API response"""\n    base_response = {\n        "success": result.error is None,\n        "model_used": result.model_name,\n        "tier_used": result.tier.value,\n        "response_time_ms": result.response_time_ms,\n        "cost_estimate": result.cost_estimate,\n        "error": result.error,\n        "fallback_used": result.fallback_used\n    }\n    \n    if result.error:\n        return response_class(**base_response)\n    \n    return base_response\n\n# API Endpoints\n\n@router.post("/detect-objects", response_model=ObjectDetectionResponse)\nasync def detect_objects(\n    request: ObjectDetectionRequest = None,\n    file: UploadFile = File(None),\n    preferred_tier: str = Form("auto"),\n    confidence_threshold: float = Form(0.5)\n):\n    """Detect objects in an image using multi-tier models"""\n    try:\n        # Get image from either request body or uploaded file\n        if file:\n            image_data = await file.read()\n            image = Image.open(io.BytesIO(image_data))\n        elif request and request.image_base64:\n            image = decode_base64_image(request.image_base64)\n        else:\n            raise HTTPException(status_code=400, detail="No image provided")\n        \n        # Configure detection\n        tier = get_model_tier(preferred_tier if file else request.preferred_tier)\n        \n        # Perform detection\n        result = await ml_service.detect_objects(\n            image,\n            preferred_tier=tier,\n            confidence_threshold=confidence_threshold if file else request.confidence_threshold\n        )\n        \n        # Format response\n        response_data = format_ml_response(result, ObjectDetectionResponse)\n        \n        if not result.error and hasattr(result.response, 'predictions'):\n            detections = []\n            for pred in result.response.predictions:\n                detections.append(DetectionResult(\n                    class_name=pred.get('class', 'unknown'),\n                    confidence=pred.get('confidence', 0.0),\n                    bbox=pred.get('bbox', {})\n                ))\n            response_data['detections'] = detections\n        \n        return ObjectDetectionResponse(**response_data)\n        \n    except Exception as e:\n        logger.error(f"Object detection failed: {e}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.post("/classify-image", response_model=ImageClassificationResponse)\nasync def classify_image(\n    request: ImageClassificationRequest = None,\n    file: UploadFile = File(None),\n    preferred_tier: str = Form("auto"),\n    top_k: int = Form(5)\n):\n    """Classify an image using multi-tier models"""\n    try:\n        # Get image\n        if file:\n            image_data = await file.read()\n            image = Image.open(io.BytesIO(image_data))\n        elif request and request.image_base64:\n            image = decode_base64_image(request.image_base64)\n        else:\n            raise HTTPException(status_code=400, detail="No image provided")\n        \n        # Configure classification\n        tier = get_model_tier(preferred_tier if file else request.preferred_tier)\n        \n        # Perform classification\n        result = await ml_service.classify_image(\n            image,\n            preferred_tier=tier,\n            top_k=top_k if file else request.top_k\n        )\n        \n        # Format response\n        response_data = format_ml_response(result, ImageClassificationResponse)\n        \n        if not result.error and hasattr(result.response, 'predictions'):\n            classifications = []\n            for pred in result.response.predictions:\n                classifications.append(ClassificationResult(\n                    class_name=pred.get('class', 'unknown'),\n                    confidence=pred.get('confidence', 0.0),\n                    class_id=pred.get('class_id')\n                ))\n            response_data['classifications'] = classifications\n        \n        return ImageClassificationResponse(**response_data)\n        \n    except Exception as e:\n        logger.error(f"Image classification failed: {e}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.post("/estimate-pose", response_model=PoseEstimationResponse)\nasync def estimate_pose(\n    request: PoseEstimationRequest = None,\n    file: UploadFile = File(None),\n    preferred_tier: str = Form("auto")\n):\n    """Estimate human pose in an image"""\n    try:\n        # Get image\n        if file:\n            image_data = await file.read()\n            image = Image.open(io.BytesIO(image_data))\n        elif request and request.image_base64:\n            image = decode_base64_image(request.image_base64)\n        else:\n            raise HTTPException(status_code=400, detail="No image provided")\n        \n        # Configure pose estimation\n        tier = get_model_tier(preferred_tier if file else request.preferred_tier)\n        \n        # Perform pose estimation\n        result = await ml_service.estimate_pose(\n            image,\n            preferred_tier=tier\n        )\n        \n        # Format response\n        response_data = format_ml_response(result, PoseEstimationResponse)\n        \n        if not result.error and hasattr(result.response, 'predictions'):\n            poses = []\n            for pred in result.response.predictions:\n                if 'keypoints' in pred:\n                    keypoints = []\n                    for kp in pred['keypoints']:\n                        keypoints.append(KeyPoint(\n                            x=kp.get('x', 0.0),\n                            y=kp.get('y', 0.0),\n                            confidence=kp.get('confidence', 0.0)\n                        ))\n                    poses.append(PoseResult(\n                        keypoints=keypoints,\n                        pose_confidence=pred.get('confidence', 0.0)\n                    ))\n            response_data['poses'] = poses\n        \n        return PoseEstimationResponse(**response_data)\n        \n    except Exception as e:\n        logger.error(f"Pose estimation failed: {e}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.post("/analyze-text", response_model=TextAnalysisResponse)\nasync def analyze_text(request: TextAnalysisRequest):\n    """Analyze text using LLM models"""\n    try:\n        # Prepare LLM input\n        llm_input = {\n            "prompt": request.prompt or f"Analyze the following text: {request.text}",\n            "max_tokens": request.max_tokens\n        }\n        \n        tier = get_model_tier(request.preferred_tier)\n        \n        # Perform analysis\n        result = await ml_service.analyze_with_llm(\n            llm_input,\n            preferred_tier=tier\n        )\n        \n        # Format response\n        response_data = format_ml_response(result, TextAnalysisResponse)\n        \n        if not result.error:\n            if hasattr(result.response, 'content'):\n                response_data['analysis'] = result.response.content\n            elif hasattr(result.response, 'predictions'):\n                response_data['analysis'] = str(result.response.predictions[0])\n            else:\n                response_data['analysis'] = str(result.response)\n            \n            # Extract token usage if available\n            if hasattr(result.response, 'usage'):\n                if hasattr(result.response.usage, 'total_tokens'):\n                    response_data['tokens_used'] = result.response.usage.total_tokens\n                elif isinstance(result.response.usage, dict):\n                    response_data['tokens_used'] = result.response.usage.get('input_tokens', 0) + result.response.usage.get('output_tokens', 0)\n        \n        return TextAnalysisResponse(**response_data)\n        \n    except Exception as e:\n        logger.error(f"Text analysis failed: {e}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.post("/generate-embeddings", response_model=EmbeddingResponse)\nasync def generate_embeddings(request: EmbeddingRequest):\n    """Generate embeddings for text"""\n    try:\n        tier = get_model_tier(request.preferred_tier)\n        \n        # Generate embeddings\n        result = await ml_service.generate_embeddings(\n            request.texts,\n            preferred_tier=tier\n        )\n        \n        # Format response\n        response_data = format_ml_response(result, EmbeddingResponse)\n        \n        if not result.error:\n            if hasattr(result.response, 'embeddings'):\n                embeddings = result.response.embeddings\n            elif hasattr(result.response, 'predictions'):\n                embeddings = result.response.predictions\n            else:\n                embeddings = []\n            \n            response_data['embeddings'] = embeddings\n            response_data['dimensions'] = len(embeddings[0]) if embeddings else 0\n        \n        return EmbeddingResponse(**response_data)\n        \n    except Exception as e:\n        logger.error(f"Embedding generation failed: {e}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.get("/models", response_model=List[ModelInfo])\nasync def list_models(category: Optional[str] = None, tier: Optional[str] = None):\n    """List available models"""\n    try:\n        models = []\n        \n        # Filter models\n        all_models = model_registry.models.values()\n        \n        if category:\n            all_models = [m for m in all_models if m.category.value == category.lower()]\n        \n        if tier:\n            all_models = [m for m in all_models if m.tier.value == tier.lower()]\n        \n        # Format model info\n        for model in all_models:\n            models.append(ModelInfo(\n                name=model.name,\n                version=model.version,\n                provider=model.provider.value,\n                category=model.category.value,\n                tier=model.tier.value,\n                description=model.description,\n                loaded=model.name in ml_service.router.loaded_clients,\n                within_rate_limits=model_registry.check_rate_limits(model.name)\n            ))\n        \n        return models\n        \n    except Exception as e:\n        logger.error(f"Failed to list models: {e}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.get("/status", response_model=ServiceStatus)\nasync def get_service_status():\n    """Get ML service status"""\n    try:\n        stats = ml_service.get_service_stats()\n        \n        return ServiceStatus(\n            total_models=stats['registry_status']['total_models'],\n            loaded_models=stats['registry_status']['loaded_models'],\n            models_by_tier=stats['registry_status']['models_by_tier'],\n            models_by_category=stats['registry_status']['models_by_category'],\n            routing_strategy=stats['config']['routing_strategy'],\n            gpu_available=stats['config']['gpu_acceleration'],\n            cache_enabled=stats['config']['cache_enabled']\n        )\n        \n    except Exception as e:\n        logger.error(f"Failed to get service status: {e}")\n        raise HTTPException(status_code=500, detail=str(e))\n\n@router.post("/configure")\nasync def configure_service(\n    routing_strategy: Optional[str] = None,\n    api_keys: Optional[Dict[str, str]] = None\n):\n    """Configure ML service settings"""\n    try:\n        if routing_strategy:\n            try:\n                strategy = RoutingStrategy(routing_strategy.lower())\n                ml_service.router.config.strategy = strategy\n                logger.info(f"Updated routing strategy to: {strategy.value}")\n            except ValueError:\n                raise HTTPException(status_code=400, detail=f"Invalid routing strategy: {routing_strategy}")\n        \n        if api_keys:\n            ml_service.router.set_api_keys(api_keys)\n            logger.info(f"Updated API keys for providers: {list(api_keys.keys())}")\n        \n        return {"success": True, "message": "Configuration updated successfully"}\n        \n    except Exception as e:\n        logger.error(f"Failed to configure service: {e}")\n        raise HTTPException(status_code=500, detail=str(e))