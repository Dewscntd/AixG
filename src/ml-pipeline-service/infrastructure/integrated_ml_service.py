"""
Integrated ML Service - Connects multi-tier models to existing pipeline
Enhances existing stages with actual model implementations
"""

import asyncio
import logging
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass
import numpy as np
from PIL import Image

from .model_registry import ModelCategory, ModelType, model_registry
from .model_tier_router import model_router, RoutingStrategy, RoutingConfig
from ..stages.base_stage import BaseStage, StageResult
from ..domain.entities.pipeline import PipelineStage

logger = logging.getLogger(__name__)

@dataclass
class MLServiceConfig:
    """Configuration for the integrated ML service"""
    default_routing_strategy: RoutingStrategy = RoutingStrategy.COST_OPTIMIZED
    enable_gpu_acceleration: bool = True
    max_concurrent_requests: int = 4
    fallback_enabled: bool = True
    cache_enabled: bool = True
    cost_threshold: float = 1.0
    api_keys: Dict[str, str] = None

class IntegratedMLService:\n    """Main service that integrates multi-tier models with the existing pipeline"""\n    \n    def __init__(self, config: MLServiceConfig = None):\n        self.config = config or MLServiceConfig()\n        self.router = model_router\n        self._setup_router()\n        self._setup_api_keys()\n        \n    def _setup_router(self):\n        """Configure the model router"""\n        routing_config = RoutingConfig(\n            strategy=self.config.default_routing_strategy,\n            enable_caching=self.config.cache_enabled,\n            cost_threshold=self.config.cost_threshold\n        )\n        self.router.config = routing_config\n        \n    def _setup_api_keys(self):\n        """Setup API keys for external providers"""\n        if self.config.api_keys:\n            self.router.set_api_keys(self.config.api_keys)\n    \n    async def detect_objects(self, input_data: Union[np.ndarray, Image.Image, str], **kwargs) -> Any:\n        """Object detection using multi-tier models"""\n        return await self.router.route_request(\n            category=ModelCategory.OBJECT_DETECTION,\n            input_data=input_data,\n            **kwargs\n        )\n    \n    async def classify_image(self, input_data: Union[np.ndarray, Image.Image, str], **kwargs) -> Any:\n        """Image classification using multi-tier models"""\n        return await self.router.route_request(\n            category=ModelCategory.COMPUTER_VISION,\n            input_data=input_data,\n            **kwargs\n        )\n    \n    async def estimate_pose(self, input_data: Union[np.ndarray, Image.Image, str], **kwargs) -> Any:\n        """Pose estimation using multi-tier models"""\n        return await self.router.route_request(\n            category=ModelCategory.POSE_ESTIMATION,\n            input_data=input_data,\n            **kwargs\n        )\n    \n    async def recognize_actions(self, input_data: Union[np.ndarray, Image.Image, str], **kwargs) -> Any:\n        """Action recognition using multi-tier models"""\n        return await self.router.route_request(\n            category=ModelCategory.ACTION_RECOGNITION,\n            input_data=input_data,\n            **kwargs\n        )\n    \n    async def track_objects(self, input_data: Union[np.ndarray, Image.Image, str], **kwargs) -> Any:\n        """Object tracking using multi-tier models"""\n        return await self.router.route_request(\n            category=ModelCategory.TRACKING,\n            input_data=input_data,\n            **kwargs\n        )\n    \n    async def generate_embeddings(self, input_data: Union[str, List[str]], **kwargs) -> Any:\n        """Generate embeddings using multi-tier models"""\n        return await self.router.route_request(\n            category=ModelCategory.EMBEDDING,\n            input_data=input_data,\n            **kwargs\n        )\n    \n    async def analyze_with_llm(self, input_data: Union[str, Dict[str, Any]], **kwargs) -> Any:\n        """Analyze using LLM models"""\n        return await self.router.route_request(\n            category=ModelCategory.LLM,\n            input_data=input_data,\n            **kwargs\n        )\n    \n    async def batch_process(self, category: ModelCategory, input_batch: List[Any], **kwargs) -> List[Any]:\n        """Process multiple inputs efficiently"""\n        return await self.router.batch_route_requests(\n            category=category,\n            input_batch=input_batch,\n            **kwargs\n        )\n    \n    def get_available_models(self, category: Optional[ModelCategory] = None, tier: Optional[ModelType] = None) -> List[str]:\n        """Get list of available models"""\n        if category:\n            models = model_registry.get_models_by_category(category)\n        else:\n            models = list(model_registry.models.values())\n        \n        if tier:\n            models = [m for m in models if m.tier == tier]\n        \n        return [m.name for m in models]\n    \n    def get_service_stats(self) -> Dict[str, Any]:\n        """Get comprehensive service statistics"""\n        return {\n            "config": {\n                "routing_strategy": self.config.default_routing_strategy.value,\n                "gpu_acceleration": self.config.enable_gpu_acceleration,\n                "cache_enabled": self.config.cache_enabled,\n                "cost_threshold": self.config.cost_threshold\n            },\n            "router_stats": self.router.get_routing_stats(),\n            "registry_status": model_registry.get_model_status()\n        }\n    \n    async def shutdown(self):\n        """Shutdown the service and cleanup resources"""\n        await self.router.unload_all_clients()\n        logger.info("ML Service shutdown complete")\n\n# Enhanced Pipeline Stages with Actual Models\n\nclass EnhancedPlayerDetectionStage(BaseStage):\n    """Enhanced player detection stage using multi-tier models"""\n    \n    def __init__(self, ml_service: IntegratedMLService, stage_config: Dict[str, Any]):\n        super().__init__()\n        self.ml_service = ml_service\n        self.stage_config = stage_config\n        \n    async def process(self, input_data: Dict[str, Any], config: Dict[str, Any]) -> StageResult:\n        \"\"\"Process video frame for player detection\"\"\"\n        try:\n            frame = input_data.get('frame')\n            if frame is None:\n                return StageResult(\n                    success=False,\n                    output_data={},\n                    error_message="No frame provided for player detection",\n                    stage_name="player_detection"\n                )\n            \n            # Configure for player detection\n            detection_config = {\n                "preferred_tier": ModelType(self.stage_config.get('preferred_tier', 'local')),\n                "confidence_threshold": self.stage_config.get('confidence_threshold', 0.5)\n            }\n            \n            # Detect objects/players\n            result = await self.ml_service.detect_objects(frame, **detection_config)\n            \n            if result.error:\n                return StageResult(\n                    success=False,\n                    output_data={},\n                    error_message=f"Player detection failed: {result.error}",\n                    stage_name="player_detection"\n                )\n            \n            # Filter for person/player detections\n            players = []\n            if hasattr(result.response, 'predictions'):\n                for detection in result.response.predictions:\n                    if detection.get('class', '').lower() in ['person', 'player', 'human']:\n                        players.append({\n                            "id": len(players),\n                            "bbox": detection.get('bbox', {}),\n                            "confidence": detection.get('confidence', 0.0),\n                            "class": detection.get('class', 'player')\n                        })\n            \n            return StageResult(\n                success=True,\n                output_data={\n                    "players": players,\n                    "frame_id": input_data.get('frame_id'),\n                    "timestamp": input_data.get('timestamp'),\n                    "model_used": result.model_name,\n                    "tier_used": result.tier.value,\n                    "response_time_ms": result.response_time_ms,\n                    "cost": result.cost_estimate\n                },\n                stage_name="player_detection",\n                metadata={\n                    "total_detections": len(players),\n                    "model_tier": result.tier.value,\n                    "fallback_used": result.fallback_used\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f"Player detection stage failed: {e}")\n            return StageResult(\n                success=False,\n                output_data={},\n                error_message=str(e),\n                stage_name="player_detection"\n            )\n\nclass EnhancedBallTrackingStage(BaseStage):\n    """Enhanced ball tracking stage using multi-tier models"""\n    \n    def __init__(self, ml_service: IntegratedMLService, stage_config: Dict[str, Any]):\n        super().__init__()\n        self.ml_service = ml_service\n        self.stage_config = stage_config\n        \n    async def process(self, input_data: Dict[str, Any], config: Dict[str, Any]) -> StageResult:\n        \"\"\"Process video frame for ball tracking\"\"\"\n        try:\n            frame = input_data.get('frame')\n            previous_ball_position = input_data.get('previous_ball_position')\n            \n            if frame is None:\n                return StageResult(\n                    success=False,\n                    output_data={},\n                    error_message="No frame provided for ball tracking",\n                    stage_name="ball_tracking"\n                )\n            \n            # Configure for ball detection\n            detection_config = {\n                "preferred_tier": ModelType(self.stage_config.get('preferred_tier', 'local')),\n                "confidence_threshold": self.stage_config.get('ball_confidence_threshold', 0.3)\n            }\n            \n            # Detect objects including ball\n            result = await self.ml_service.detect_objects(frame, **detection_config)\n            \n            if result.error:\n                return StageResult(\n                    success=False,\n                    output_data={},\n                    error_message=f"Ball tracking failed: {result.error}",\n                    stage_name="ball_tracking"\n                )\n            \n            # Filter for ball detections\n            ball_candidates = []\n            if hasattr(result.response, 'predictions'):\n                for detection in result.response.predictions:\n                    if detection.get('class', '').lower() in ['ball', 'sports ball', 'soccer ball', 'football']:\n                        ball_candidates.append({\n                            "bbox": detection.get('bbox', {}),\n                            "confidence": detection.get('confidence', 0.0),\n                            "center": self._get_bbox_center(detection.get('bbox', {}))\n                        })\n            \n            # Select best ball candidate (highest confidence, or closest to previous position)\n            ball_position = None\n            if ball_candidates:\n                if previous_ball_position:\n                    # Choose closest to previous position\n                    best_candidate = min(ball_candidates, \n                                       key=lambda x: self._distance(x['center'], previous_ball_position))\n                else:\n                    # Choose highest confidence\n                    best_candidate = max(ball_candidates, key=lambda x: x['confidence'])\n                \n                ball_position = {\n                    "center": best_candidate['center'],\n                    "bbox": best_candidate['bbox'],\n                    "confidence": best_candidate['confidence']\n                }\n            \n            return StageResult(\n                success=True,\n                output_data={\n                    "ball_position": ball_position,\n                    "ball_candidates": ball_candidates,\n                    "frame_id": input_data.get('frame_id'),\n                    "timestamp": input_data.get('timestamp'),\n                    "model_used": result.model_name,\n                    "tier_used": result.tier.value,\n                    "response_time_ms": result.response_time_ms,\n                    "cost": result.cost_estimate\n                },\n                stage_name="ball_tracking",\n                metadata={\n                    "ball_detected": ball_position is not None,\n                    "candidates_found": len(ball_candidates),\n                    "model_tier": result.tier.value\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f"Ball tracking stage failed: {e}")\n            return StageResult(\n                success=False,\n                output_data={},\n                error_message=str(e),\n                stage_name="ball_tracking"\n            )\n    \n    def _get_bbox_center(self, bbox: Dict[str, float]) -> Tuple[float, float]:\n        \"\"\"Get center point of bounding box\"\"\"\n        x = bbox.get('x', 0)\n        y = bbox.get('y', 0)\n        width = bbox.get('width', 0)\n        height = bbox.get('height', 0)\n        return (x + width / 2, y + height / 2)\n    \n    def _distance(self, point1: Tuple[float, float], point2: Tuple[float, float]) -> float:\n        \"\"\"Calculate Euclidean distance between two points\"\"\"\n        return ((point1[0] - point2[0]) ** 2 + (point1[1] - point2[1]) ** 2) ** 0.5\n\nclass EnhancedActionRecognitionStage(BaseStage):\n    """Enhanced action recognition stage using multi-tier models"""\n    \n    def __init__(self, ml_service: IntegratedMLService, stage_config: Dict[str, Any]):\n        super().__init__()\n        self.ml_service = ml_service\n        self.stage_config = stage_config\n        \n    async def process(self, input_data: Dict[str, Any], config: Dict[str, Any]) -> StageResult:\n        \"\"\"Process video frame sequence for action recognition\"\"\"\n        try:\n            frames = input_data.get('frames', [])\n            players = input_data.get('players', [])\n            \n            if not frames:\n                return StageResult(\n                    success=False,\n                    output_data={},\n                    error_message="No frames provided for action recognition",\n                    stage_name="action_recognition"\n                )\n            \n            actions = []\n            \n            # Process each player's actions\n            for player in players:\n                try:\n                    # Extract player region from frames (if bbox available)\n                    player_frames = self._extract_player_regions(frames, player)\n                    \n                    if player_frames:\n                        # Recognize actions\n                        result = await self.ml_service.recognize_actions(player_frames)\n                        \n                        if not result.error and hasattr(result.response, 'predictions'):\n                            for prediction in result.response.predictions:\n                                actions.append({\n                                    "player_id": player.get('id'),\n                                    "action": prediction.get('class', 'unknown'),\n                                    "confidence": prediction.get('confidence', 0.0),\n                                    "timestamp": input_data.get('timestamp'),\n                                    "model_used": result.model_name\n                                })\n                \n                except Exception as e:\n                    logger.warning(f"Action recognition failed for player {player.get('id')}: {e}")\n                    continue\n            \n            return StageResult(\n                success=True,\n                output_data={\n                    "actions": actions,\n                    "frame_sequence_id": input_data.get('frame_sequence_id'),\n                    "timestamp": input_data.get('timestamp')\n                },\n                stage_name="action_recognition",\n                metadata={\n                    "total_actions": len(actions),\n                    "players_processed": len(players)\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f"Action recognition stage failed: {e}")\n            return StageResult(\n                success=False,\n                output_data={},\n                error_message=str(e),\n                stage_name="action_recognition"\n            )\n    \n    def _extract_player_regions(self, frames: List[np.ndarray], player: Dict[str, Any]) -> List[np.ndarray]:\n        \"\"\"Extract player regions from frame sequence\"\"\"\n        player_regions = []\n        bbox = player.get('bbox')\n        \n        if bbox:\n            for frame in frames:\n                try:\n                    x, y, w, h = int(bbox['x']), int(bbox['y']), int(bbox['width']), int(bbox['height'])\n                    player_region = frame[y:y+h, x:x+w]\n                    if player_region.size > 0:\n                        player_regions.append(player_region)\n                except Exception:\n                    continue\n        \n        return player_regions\n\n# Global ML service instance\nml_service = IntegratedMLService()\n\n# Factory functions for enhanced stages\ndef create_enhanced_player_detection_stage(config: Dict[str, Any]) -> EnhancedPlayerDetectionStage:\n    \"\"\"Create enhanced player detection stage\"\"\"\n    return EnhancedPlayerDetectionStage(ml_service, config)\n\ndef create_enhanced_ball_tracking_stage(config: Dict[str, Any]) -> EnhancedBallTrackingStage:\n    \"\"\"Create enhanced ball tracking stage\"\"\"\n    return EnhancedBallTrackingStage(ml_service, config)\n\ndef create_enhanced_action_recognition_stage(config: Dict[str, Any]) -> EnhancedActionRecognitionStage:\n    \"\"\"Create enhanced action recognition stage\"\"\"\n    return EnhancedActionRecognitionStage(ml_service, config)