"""
Hugging Face Hub API Client for Free ML Models
Supports inference API and transformers models
"""

import asyncio
import base64
import io
import logging
from typing import Any, Dict, List, Optional, Union
from dataclasses import dataclass
from PIL import Image
import numpy as np

import aiohttp
import torch
from transformers import AutoTokenizer, AutoModel, AutoProcessor, pipeline
from huggingface_hub import AsyncInferenceClient

from ..model_registry import ModelInterface, ModelMetadata

logger = logging.getLogger(__name__)

@dataclass
class HuggingFaceResponse:
    predictions: List[Any]
    model: str
    response_time_ms: float
    confidence_scores: Optional[List[float]] = None

class HuggingFaceInferenceClient(ModelInterface):
    """Hugging Face Inference API client for free models"""
    
    def __init__(self, metadata: ModelMetadata, api_key: Optional[str] = None):
        self.metadata = metadata
        self.api_key = api_key
        self.client = AsyncInferenceClient(token=api_key) if api_key else AsyncInferenceClient()
        self._loaded = False
        
    async def load(self) -> None:
        """Initialize the client"""
        try:
            # Test the connection by checking model info
            model_url = self.metadata.endpoint_url or f"https://api-inference.huggingface.co/models/{self.metadata.name}"
            self.model_url = model_url
            self._loaded = True
            logger.info(f"Hugging Face Inference client loaded: {self.metadata.name}")
        except Exception as e:
            logger.error(f"Failed to initialize Hugging Face client: {e}")
            raise
    
    def is_loaded(self) -> bool:
        return self._loaded
    
    async def unload(self) -> None:
        """Close the client"""
        self._loaded = False
        logger.info(f"Hugging Face client unloaded: {self.metadata.name}")
    
    async def predict(self, input_data: Union[str, np.ndarray, Image.Image, Dict[str, Any]]) -> HuggingFaceResponse:
        """Make prediction using Hugging Face Inference API"""
        if not self._loaded:
            await self.load()
        
        try:\n            import time\n            start_time = time.time()\n            \n            # Process input based on model category\n            if self.metadata.category.value == "object_detection":\n                result = await self._object_detection_predict(input_data)\n            elif self.metadata.category.value == "computer_vision":\n                result = await self._image_classification_predict(input_data)\n            elif self.metadata.category.value == "embedding":\n                result = await self._embedding_predict(input_data)\n            elif self.metadata.category.value == "llm":\n                result = await self._text_generation_predict(input_data)\n            else:\n                result = await self._generic_predict(input_data)\n            \n            response_time_ms = (time.time() - start_time) * 1000\n            \n            return HuggingFaceResponse(\n                predictions=result.get('predictions', [result]),\n                model=self.metadata.name,\n                response_time_ms=response_time_ms,\n                confidence_scores=result.get('confidence_scores')\n            )\n            \n        except Exception as e:\n            logger.error(f"Hugging Face prediction failed: {e}")\n            raise\n    \n    async def _object_detection_predict(self, input_data: Union[np.ndarray, Image.Image, str]) -> Dict[str, Any]:\n        """Object detection prediction"""\n        image_bytes = self._prepare_image_bytes(input_data)\n        \n        async with aiohttp.ClientSession() as session:\n            headers = {"Authorization": f"Bearer {self.api_key}"} if self.api_key else {}\n            \n            async with session.post(\n                self.model_url,\n                headers=headers,\n                data=image_bytes\n            ) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    \n                    # Convert HF format to standard detection format\n                    predictions = []\n                    confidence_scores = []\n                    \n                    for detection in result:\n                        predictions.append({\n                            "class": detection.get("label", "unknown"),\n                            "confidence": detection.get("score", 0.0),\n                            "bbox": {\n                                "x": detection.get("box", {}).get("xmin", 0),\n                                "y": detection.get("box", {}).get("ymin", 0),\n                                "width": detection.get("box", {}).get("xmax", 0) - detection.get("box", {}).get("xmin", 0),\n                                "height": detection.get("box", {}).get("ymax", 0) - detection.get("box", {}).get("ymin", 0)\n                            }\n                        })\n                        confidence_scores.append(detection.get("score", 0.0))\n                    \n                    return {\n                        "predictions": predictions,\n                        "confidence_scores": confidence_scores\n                    }\n                else:\n                    error_text = await response.text()\n                    raise Exception(f"API request failed: {response.status} - {error_text}")\n    \n    async def _image_classification_predict(self, input_data: Union[np.ndarray, Image.Image, str]) -> Dict[str, Any]:\n        """Image classification prediction"""\n        image_bytes = self._prepare_image_bytes(input_data)\n        \n        async with aiohttp.ClientSession() as session:\n            headers = {"Authorization": f"Bearer {self.api_key}"} if self.api_key else {}\n            \n            async with session.post(\n                self.model_url,\n                headers=headers,\n                data=image_bytes\n            ) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    \n                    predictions = []\n                    confidence_scores = []\n                    \n                    for prediction in result:\n                        predictions.append({\n                            "class": prediction.get("label", "unknown"),\n                            "confidence": prediction.get("score", 0.0)\n                        })\n                        confidence_scores.append(prediction.get("score", 0.0))\n                    \n                    return {\n                        "predictions": predictions,\n                        "confidence_scores": confidence_scores\n                    }\n                else:\n                    error_text = await response.text()\n                    raise Exception(f"API request failed: {response.status} - {error_text}")\n    \n    async def _embedding_predict(self, input_data: Union[str, List[str]]) -> Dict[str, Any]:\n        """Text embedding prediction"""\n        if isinstance(input_data, str):\n            texts = [input_data]\n        else:\n            texts = input_data\n        \n        async with aiohttp.ClientSession() as session:\n            headers = {\n                "Authorization": f"Bearer {self.api_key}",\n                "Content-Type": "application/json"\n            } if self.api_key else {"Content-Type": "application/json"}\n            \n            payload = {"inputs": texts}\n            \n            async with session.post(\n                self.model_url,\n                headers=headers,\n                json=payload\n            ) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    return {"predictions": result}\n                else:\n                    error_text = await response.text()\n                    raise Exception(f"API request failed: {response.status} - {error_text}")\n    \n    async def _text_generation_predict(self, input_data: Union[str, Dict[str, Any]]) -> Dict[str, Any]:\n        """Text generation prediction"""\n        if isinstance(input_data, str):\n            prompt = input_data\n            max_length = 100\n            temperature = 0.7\n        else:\n            prompt = input_data.get("prompt", "")\n            max_length = input_data.get("max_length", 100)\n            temperature = input_data.get("temperature", 0.7)\n        \n        async with aiohttp.ClientSession() as session:\n            headers = {\n                "Authorization": f"Bearer {self.api_key}",\n                "Content-Type": "application/json"\n            } if self.api_key else {"Content-Type": "application/json"}\n            \n            payload = {\n                "inputs": prompt,\n                "parameters": {\n                    "max_length": max_length,\n                    "temperature": temperature,\n                    "return_full_text": False\n                }\n            }\n            \n            async with session.post(\n                self.model_url,\n                headers=headers,\n                json=payload\n            ) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    \n                    if isinstance(result, list) and len(result) > 0:\n                        generated_text = result[0].get("generated_text", "")\n                    else:\n                        generated_text = str(result)\n                    \n                    return {"predictions": [{"generated_text": generated_text}]}\n                else:\n                    error_text = await response.text()\n                    raise Exception(f"API request failed: {response.status} - {error_text}")\n    \n    async def _generic_predict(self, input_data: Any) -> Dict[str, Any]:\n        """Generic prediction for unknown model types"""\n        async with aiohttp.ClientSession() as session:\n            headers = {"Authorization": f"Bearer {self.api_key}"} if self.api_key else {}\n            \n            # Try to determine if it's image or text data\n            if isinstance(input_data, (np.ndarray, Image.Image)) or (isinstance(input_data, str) and input_data.endswith(('.jpg', '.png', '.jpeg'))):\n                data = self._prepare_image_bytes(input_data)\n                content_type = None\n            else:\n                data = {"inputs": str(input_data)}\n                headers["Content-Type"] = "application/json"\n                content_type = "json"\n            \n            async with session.post(\n                self.model_url,\n                headers=headers,\n                json=data if content_type == "json" else None,\n                data=data if content_type != "json" else None\n            ) as response:\n                if response.status == 200:\n                    result = await response.json()\n                    return {"predictions": [result] if not isinstance(result, list) else result}\n                else:\n                    error_text = await response.text()\n                    raise Exception(f"API request failed: {response.status} - {error_text}")\n    \n    def _prepare_image_bytes(self, image: Union[np.ndarray, Image.Image, str]) -> bytes:\n        """Convert image to bytes for API request"""\n        if isinstance(image, str):\n            if image.startswith('/'):\n                # File path\n                with open(image, 'rb') as f:\n                    return f.read()\n            else:\n                # Assume base64\n                return base64.b64decode(image)\n        \n        elif isinstance(image, np.ndarray):\n            image = Image.fromarray(image.astype('uint8'))\n        \n        if isinstance(image, Image.Image):\n            buffer = io.BytesIO()\n            image.save(buffer, format='JPEG')\n            return buffer.getvalue()\n        \n        raise ValueError(f"Unsupported image type: {type(image)}")\n    \n    async def batch_predict(self, input_batch: List[Any]) -> List[HuggingFaceResponse]:\n        """Make batch predictions"""\n        results = []\n        for input_data in input_batch:\n            try:\n                result = await self.predict(input_data)\n                results.append(result)\n                # Small delay to respect rate limits\n                await asyncio.sleep(0.2)\n            except Exception as e:\n                logger.error(f"Batch prediction failed for item: {e}")\n                results.append(None)\n        return results\n\nclass HuggingFaceLocalClient(ModelInterface):\n    """Local Hugging Face model client using transformers"""\n    \n    def __init__(self, metadata: ModelMetadata):\n        self.metadata = metadata\n        self.model = None\n        self.tokenizer = None\n        self.processor = None\n        self.pipeline = None\n        self._loaded = False\n        \n    async def load(self) -> None:\n        """Load the model locally"""\n        try:\n            model_name = self.metadata.name\n            \n            if self.metadata.category.value == "object_detection":\n                self.pipeline = pipeline(\n                    "object-detection",\n                    model=model_name,\n                    device=0 if torch.cuda.is_available() else -1\n                )\n            elif self.metadata.category.value == "computer_vision":\n                self.pipeline = pipeline(\n                    "image-classification",\n                    model=model_name,\n                    device=0 if torch.cuda.is_available() else -1\n                )\n            elif self.metadata.category.value == "embedding":\n                self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n                self.model = AutoModel.from_pretrained(model_name)\n                if torch.cuda.is_available():\n                    self.model = self.model.cuda()\n            elif self.metadata.category.value == "llm":\n                self.pipeline = pipeline(\n                    "text-generation",\n                    model=model_name,\n                    device=0 if torch.cuda.is_available() else -1\n                )\n            else:\n                # Try to auto-detect the task\n                self.pipeline = pipeline(\n                    model=model_name,\n                    device=0 if torch.cuda.is_available() else -1\n                )\n            \n            self._loaded = True\n            logger.info(f"Local Hugging Face model loaded: {self.metadata.name}")\n            \n        except Exception as e:\n            logger.error(f"Failed to load local Hugging Face model: {e}")\n            raise\n    \n    def is_loaded(self) -> bool:\n        return self._loaded\n    \n    async def unload(self) -> None:\n        """Unload the model"""\n        if self.model:\n            del self.model\n        if self.tokenizer:\n            del self.tokenizer\n        if self.processor:\n            del self.processor\n        if self.pipeline:\n            del self.pipeline\n        \n        # Clear GPU cache\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        self._loaded = False\n        logger.info(f"Local Hugging Face model unloaded: {self.metadata.name}")\n    \n    async def predict(self, input_data: Any) -> HuggingFaceResponse:\n        """Make prediction using local model"""\n        if not self._loaded:\n            await self.load()\n        \n        try:\n            import time\n            start_time = time.time()\n            \n            if self.pipeline:\n                result = self.pipeline(input_data)\n            elif self.model and self.tokenizer:  # For embeddings\n                if isinstance(input_data, str):\n                    inputs = self.tokenizer(input_data, return_tensors="pt", padding=True, truncation=True)\n                    if torch.cuda.is_available():\n                        inputs = {k: v.cuda() for k, v in inputs.items()}\n                    \n                    with torch.no_grad():\n                        outputs = self.model(**inputs)\n                        embeddings = outputs.last_hidden_state.mean(dim=1)\n                        result = embeddings.cpu().numpy().tolist()\n                else:\n                    raise ValueError("Text input required for embedding models")\n            else:\n                raise ValueError("No valid model or pipeline loaded")\n            \n            response_time_ms = (time.time() - start_time) * 1000\n            \n            # Standardize the result format\n            if isinstance(result, list):\n                predictions = result\n            else:\n                predictions = [result]\n            \n            confidence_scores = None\n            if predictions and isinstance(predictions[0], dict) and 'score' in predictions[0]:\n                confidence_scores = [pred.get('score', 0.0) for pred in predictions]\n            \n            return HuggingFaceResponse(\n                predictions=predictions,\n                model=self.metadata.name,\n                response_time_ms=response_time_ms,\n                confidence_scores=confidence_scores\n            )\n            \n        except Exception as e:\n            logger.error(f"Local Hugging Face prediction failed: {e}")\n            raise\n    \n    async def batch_predict(self, input_batch: List[Any]) -> List[HuggingFaceResponse]:\n        """Make batch predictions using local model"""\n        if not self._loaded:\n            await self.load()\n        \n        try:\n            import time\n            start_time = time.time()\n            \n            if self.pipeline:\n                # Most pipelines support batch processing\n                results = self.pipeline(input_batch)\n            else:\n                # Process individually for custom models\n                results = []\n                for input_data in input_batch:\n                    result = await self.predict(input_data)\n                    results.append(result.predictions[0] if result.predictions else None)\n            \n            response_time_ms = (time.time() - start_time) * 1000\n            \n            # Convert to list of HuggingFaceResponse objects\n            responses = []\n            for i, result in enumerate(results):\n                confidence_scores = None\n                if isinstance(result, dict) and 'score' in result:\n                    confidence_scores = [result.get('score', 0.0)]\n                \n                responses.append(HuggingFaceResponse(\n                    predictions=[result] if result else [],\n                    model=self.metadata.name,\n                    response_time_ms=response_time_ms / len(results),  # Approximate per-item time\n                    confidence_scores=confidence_scores\n                ))\n            \n            return responses\n            \n        except Exception as e:\n            logger.error(f"Local Hugging Face batch prediction failed: {e}")\n            raise\n\ndef create_huggingface_client(metadata: ModelMetadata, api_key: Optional[str] = None, use_local: bool = False) -> ModelInterface:\n    """Factory function to create appropriate Hugging Face client"""\n    if use_local or metadata.tier.value == "local":\n        return HuggingFaceLocalClient(metadata)\n    else:\n        return HuggingFaceInferenceClient(metadata, api_key)