"""
Local Model Clients for PyTorch, TensorFlow, ONNX, TensorRT, and YOLO models
Supports high-performance local inference with GPU optimization
"""

import asyncio
import logging
from typing import Any, Dict, List, Optional, Union, Tuple
from dataclasses import dataclass
from pathlib import Path
import numpy as np
from PIL import Image
import io
import json

import torch
import torch.nn as nn
from torchvision import transforms
import cv2

from ..model_registry import ModelInterface, ModelMetadata

logger = logging.getLogger(__name__)

@dataclass
class LocalModelResponse:
    predictions: List[Any]
    model: str
    response_time_ms: float
    confidence_scores: Optional[List[float]] = None
    metadata: Optional[Dict[str, Any]] = None

class YOLOClient(ModelInterface):
    """YOLO model client for object detection"""
    
    def __init__(self, metadata: ModelMetadata):
        self.metadata = metadata
        self.model = None
        self._loaded = False
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
    async def load(self) -> None:
        """Load YOLO model"""
        try:
            from ultralytics import YOLO
            
            model_path = self.metadata.local_path
            if not model_path or not model_path.exists():
                # Try to download from ultralytics hub
                model_name = self.metadata.name.split('-')[0]  # e.g., yolov8n from yolov8n-free
                self.model = YOLO(f"{model_name}.pt")
            else:
                self.model = YOLO(str(model_path))
            
            self.model.to(self.device)
            self._loaded = True
            logger.info(f"YOLO model loaded: {self.metadata.name} on {self.device}")
            
        except Exception as e:
            logger.error(f"Failed to load YOLO model: {e}")
            raise
    
    def is_loaded(self) -> bool:
        return self._loaded
    
    async def unload(self) -> None:
        """Unload YOLO model"""
        if self.model:
            del self.model
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        self._loaded = False
        logger.info(f"YOLO model unloaded: {self.metadata.name}")
    
    async def predict(self, input_data: Union[np.ndarray, Image.Image, str]) -> LocalModelResponse:
        """Make prediction using YOLO"""
        if not self._loaded:
            await self.load()
        
        try:
            import time
            start_time = time.time()
            
            # YOLO can handle various input formats
            results = self.model(input_data, verbose=False)
            
            response_time_ms = (time.time() - start_time) * 1000
            
            # Convert YOLO results to standard format
            predictions = []
            confidence_scores = []
            
            for result in results:
                for box in result.boxes:\n                    predictions.append({\n                        "class": self.model.names[int(box.cls[0])],\n                        "confidence": float(box.conf[0]),\n                        "bbox": {\n                            "x": float(box.xyxy[0][0]),\n                            "y": float(box.xyxy[0][1]),\n                            "width": float(box.xyxy[0][2] - box.xyxy[0][0]),\n                            "height": float(box.xyxy[0][3] - box.xyxy[0][1])\n                        }\n                    })\n                    confidence_scores.append(float(box.conf[0]))\n            \n            return LocalModelResponse(\n                predictions=predictions,\n                model=self.metadata.name,\n                response_time_ms=response_time_ms,\n                confidence_scores=confidence_scores,\n                metadata={\n                    "input_shape": results[0].orig_shape if results else None,\n                    "device": str(self.device)\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f"YOLO prediction failed: {e}")\n            raise\n    \n    async def batch_predict(self, input_batch: List[Any]) -> List[LocalModelResponse]:\n        """Make batch predictions using YOLO"""\n        if not self._loaded:\n            await self.load()\n        \n        try:\n            import time\n            start_time = time.time()\n            \n            # YOLO supports batch processing natively\n            results = self.model(input_batch, verbose=False)\n            \n            response_time_ms = (time.time() - start_time) * 1000\n            per_item_time = response_time_ms / len(input_batch)\n            \n            responses = []\n            for i, result in enumerate(results):\n                predictions = []\n                confidence_scores = []\n                \n                for box in result.boxes:\n                    predictions.append({\n                        "class": self.model.names[int(box.cls[0])],\n                        "confidence": float(box.conf[0]),\n                        "bbox": {\n                            "x": float(box.xyxy[0][0]),\n                            "y": float(box.xyxy[0][1]),\n                            "width": float(box.xyxy[0][2] - box.xyxy[0][0]),\n                            "height": float(box.xyxy[0][3] - box.xyxy[0][1])\n                        }\n                    })\n                    confidence_scores.append(float(box.conf[0]))\n                \n                responses.append(LocalModelResponse(\n                    predictions=predictions,\n                    model=self.metadata.name,\n                    response_time_ms=per_item_time,\n                    confidence_scores=confidence_scores,\n                    metadata={\n                        "input_shape": result.orig_shape,\n                        "device": str(self.device),\n                        "batch_index": i\n                    }\n                ))\n            \n            return responses\n            \n        except Exception as e:\n            logger.error(f"YOLO batch prediction failed: {e}")\n            raise\n\nclass PyTorchClient(ModelInterface):\n    """PyTorch model client for custom models"""\n    \n    def __init__(self, metadata: ModelMetadata):\n        self.metadata = metadata\n        self.model = None\n        self.transform = None\n        self._loaded = False\n        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n        self.class_names = []\n        \n    async def load(self) -> None:\n        """Load PyTorch model"""\n        try:\n            model_path = self.metadata.local_path\n            if not model_path or not model_path.exists():\n                raise FileNotFoundError(f"Model file not found: {model_path}")\n            \n            # Load the model\n            checkpoint = torch.load(str(model_path), map_location=self.device)\n            \n            if isinstance(checkpoint, dict):\n                # Handle different checkpoint formats\n                if 'model' in checkpoint:\n                    self.model = checkpoint['model']\n                elif 'state_dict' in checkpoint:\n                    # Load architecture first (this requires model definition)\n                    self.model = self._create_model_architecture()\n                    self.model.load_state_dict(checkpoint['state_dict'])\n                else:\n                    self.model = checkpoint\n                \n                # Load class names if available\n                if 'class_names' in checkpoint:\n                    self.class_names = checkpoint['class_names']\n            else:\n                self.model = checkpoint\n            \n            self.model.to(self.device)\n            self.model.eval()\n            \n            # Setup transforms\n            self.transform = self._setup_transforms()\n            \n            self._loaded = True\n            logger.info(f"PyTorch model loaded: {self.metadata.name} on {self.device}")\n            \n        except Exception as e:\n            logger.error(f"Failed to load PyTorch model: {e}")\n            raise\n    \n    def _create_model_architecture(self):\n        """Create model architecture - override in subclasses"""\n        # This is a placeholder - in practice, you'd define the architecture\n        # based on the model type or load it from a config file\n        raise NotImplementedError("Model architecture must be defined")\n    \n    def _setup_transforms(self):\n        """Setup image transforms"""\n        preprocessing_config = self.metadata.preprocessing_config\n        \n        transform_list = []\n        \n        # Resize\n        if 'resize' in preprocessing_config:\n            size = preprocessing_config['resize']\n            transform_list.append(transforms.Resize(size))\n        \n        # Center crop\n        if 'center_crop' in preprocessing_config:\n            size = preprocessing_config['center_crop']\n            transform_list.append(transforms.CenterCrop(size))\n        \n        # Convert to tensor\n        transform_list.append(transforms.ToTensor())\n        \n        # Normalize\n        if 'normalize' in preprocessing_config:\n            mean = preprocessing_config['normalize'].get('mean', [0.485, 0.456, 0.406])\n            std = preprocessing_config['normalize'].get('std', [0.229, 0.224, 0.225])\n            transform_list.append(transforms.Normalize(mean=mean, std=std))\n        \n        return transforms.Compose(transform_list)\n    \n    def is_loaded(self) -> bool:\n        return self._loaded\n    \n    async def unload(self) -> None:\n        """Unload PyTorch model"""\n        if self.model:\n            del self.model\n            if torch.cuda.is_available():\n                torch.cuda.empty_cache()\n        self._loaded = False\n        logger.info(f"PyTorch model unloaded: {self.metadata.name}")\n    \n    async def predict(self, input_data: Union[np.ndarray, Image.Image, torch.Tensor]) -> LocalModelResponse:\n        """Make prediction using PyTorch model"""\n        if not self._loaded:\n            await self.load()\n        \n        try:\n            import time\n            start_time = time.time()\n            \n            # Prepare input\n            if isinstance(input_data, torch.Tensor):\n                tensor_input = input_data.to(self.device)\n            else:\n                # Convert to PIL Image if numpy array\n                if isinstance(input_data, np.ndarray):\n                    input_data = Image.fromarray(input_data)\n                \n                # Apply transforms\n                tensor_input = self.transform(input_data).unsqueeze(0).to(self.device)\n            \n            # Make prediction\n            with torch.no_grad():\n                outputs = self.model(tensor_input)\n            \n            response_time_ms = (time.time() - start_time) * 1000\n            \n            # Process outputs based on model category\n            if self.metadata.category.value == "pose_estimation":\n                predictions = self._process_pose_outputs(outputs)\n            elif self.metadata.category.value == "computer_vision":\n                predictions = self._process_classification_outputs(outputs)\n            else:\n                predictions = self._process_generic_outputs(outputs)\n            \n            return LocalModelResponse(\n                predictions=predictions,\n                model=self.metadata.name,\n                response_time_ms=response_time_ms,\n                metadata={\n                    "input_shape": list(tensor_input.shape),\n                    "device": str(self.device)\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f"PyTorch prediction failed: {e}")\n            raise\n    \n    def _process_pose_outputs(self, outputs: torch.Tensor) -> List[Dict[str, Any]]:\n        """Process pose estimation outputs"""\n        # Assuming outputs are keypoints [batch, num_keypoints, 3] (x, y, confidence)\n        if len(outputs.shape) == 3:\n            keypoints = outputs[0].cpu().numpy()  # Take first batch item\n            \n            return [{\n                "keypoints": [\n                    {\n                        "x": float(kp[0]),\n                        "y": float(kp[1]),\n                        "confidence": float(kp[2]) if kp.shape[0] > 2 else 1.0\n                    }\n                    for kp in keypoints\n                ],\n                "type": "pose"\n            }]\n        else:\n            return [{"raw_output": outputs.cpu().numpy().tolist()}]\n    \n    def _process_classification_outputs(self, outputs: torch.Tensor) -> List[Dict[str, Any]]:\n        """Process classification outputs"""\n        probabilities = torch.softmax(outputs, dim=1)\n        top_probs, top_indices = torch.topk(probabilities, k=5)  # Top 5 predictions\n        \n        predictions = []\n        for i in range(top_probs.shape[1]):\n            class_idx = int(top_indices[0][i])\n            confidence = float(top_probs[0][i])\n            \n            class_name = self.class_names[class_idx] if class_idx < len(self.class_names) else f"class_{class_idx}"\n            \n            predictions.append({\n                "class": class_name,\n                "confidence": confidence,\n                "class_id": class_idx\n            })\n        \n        return predictions\n    \n    def _process_generic_outputs(self, outputs: torch.Tensor) -> List[Dict[str, Any]]:\n        """Process generic model outputs"""\n        return [{"raw_output": outputs.cpu().numpy().tolist()}]\n    \n    async def batch_predict(self, input_batch: List[Any]) -> List[LocalModelResponse]:\n        """Make batch predictions using PyTorch model"""\n        if not self._loaded:\n            await self.load()\n        \n        try:\n            import time\n            start_time = time.time()\n            \n            # Prepare batch tensor\n            tensor_batch = []\n            for input_data in input_batch:\n                if isinstance(input_data, torch.Tensor):\n                    tensor_batch.append(input_data)\n                else:\n                    if isinstance(input_data, np.ndarray):\n                        input_data = Image.fromarray(input_data)\n                    tensor_batch.append(self.transform(input_data))\n            \n            batch_tensor = torch.stack(tensor_batch).to(self.device)\n            \n            # Make batch prediction\n            with torch.no_grad():\n                outputs = self.model(batch_tensor)\n            \n            response_time_ms = (time.time() - start_time) * 1000\n            per_item_time = response_time_ms / len(input_batch)\n            \n            # Process each item in the batch\n            responses = []\n            for i in range(len(input_batch)):\n                item_output = outputs[i:i+1]  # Keep batch dimension\n                \n                if self.metadata.category.value == "pose_estimation":\n                    predictions = self._process_pose_outputs(item_output)\n                elif self.metadata.category.value == "computer_vision":\n                    predictions = self._process_classification_outputs(item_output)\n                else:\n                    predictions = self._process_generic_outputs(item_output)\n                \n                responses.append(LocalModelResponse(\n                    predictions=predictions,\n                    model=self.metadata.name,\n                    response_time_ms=per_item_time,\n                    metadata={\n                        "input_shape": list(batch_tensor[i].shape),\n                        "device": str(self.device),\n                        "batch_index": i\n                    }\n                ))\n            \n            return responses\n            \n        except Exception as e:\n            logger.error(f"PyTorch batch prediction failed: {e}")\n            raise\n\nclass ONNXClient(ModelInterface):\n    """ONNX model client for cross-platform inference"""\n    \n    def __init__(self, metadata: ModelMetadata):\n        self.metadata = metadata\n        self.session = None\n        self._loaded = False\n        \n    async def load(self) -> None:\n        """Load ONNX model"""\n        try:\n            import onnxruntime as ort\n            \n            model_path = self.metadata.local_path\n            if not model_path or not model_path.exists():\n                raise FileNotFoundError(f"Model file not found: {model_path}")\n            \n            # Configure providers (GPU if available)\n            providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']\n            \n            self.session = ort.InferenceSession(str(model_path), providers=providers)\n            \n            self._loaded = True\n            logger.info(f"ONNX model loaded: {self.metadata.name}")\n            \n        except Exception as e:\n            logger.error(f"Failed to load ONNX model: {e}")\n            raise\n    \n    def is_loaded(self) -> bool:\n        return self._loaded\n    \n    async def unload(self) -> None:\n        """Unload ONNX model"""\n        if self.session:\n            del self.session\n        self._loaded = False\n        logger.info(f"ONNX model unloaded: {self.metadata.name}")\n    \n    async def predict(self, input_data: Union[np.ndarray, Image.Image]) -> LocalModelResponse:\n        """Make prediction using ONNX model"""\n        if not self._loaded:\n            await self.load()\n        \n        try:\n            import time\n            start_time = time.time()\n            \n            # Prepare input\n            if isinstance(input_data, Image.Image):\n                input_data = np.array(input_data)\n            \n            # Ensure proper shape for ONNX (NCHW format)\n            if len(input_data.shape) == 3:\n                input_data = np.expand_dims(input_data.transpose(2, 0, 1), axis=0)\n            elif len(input_data.shape) == 4 and input_data.shape[-1] in [1, 3, 4]:\n                input_data = input_data.transpose(0, 3, 1, 2)\n            \n            input_data = input_data.astype(np.float32) / 255.0  # Normalize\n            \n            # Get input name\n            input_name = self.session.get_inputs()[0].name\n            \n            # Run inference\n            outputs = self.session.run(None, {input_name: input_data})\n            \n            response_time_ms = (time.time() - start_time) * 1000\n            \n            # Process outputs\n            predictions = [{"raw_output": output.tolist()} for output in outputs]\n            \n            return LocalModelResponse(\n                predictions=predictions,\n                model=self.metadata.name,\n                response_time_ms=response_time_ms,\n                metadata={\n                    "input_shape": list(input_data.shape),\n                    "output_shapes": [list(output.shape) for output in outputs]\n                }\n            )\n            \n        except Exception as e:\n            logger.error(f"ONNX prediction failed: {e}")\n            raise\n    \n    async def batch_predict(self, input_batch: List[Any]) -> List[LocalModelResponse]:\n        """Make batch predictions using ONNX model"""\n        # Process each item individually for simplicity\n        results = []\n        for input_data in input_batch:\n            result = await self.predict(input_data)\n            results.append(result)\n        return results\n\ndef create_local_model_client(metadata: ModelMetadata) -> ModelInterface:\n    """Factory function to create appropriate local model client"""\n    if metadata.provider.value == "yolo":\n        return YOLOClient(metadata)\n    elif metadata.provider.value == "pytorch":\n        return PyTorchClient(metadata)\n    elif metadata.provider.value == "onnx":\n        return ONNXClient(metadata)\n    elif metadata.provider.value == "tensorflow":\n        # TODO: Implement TensorFlow client\n        raise NotImplementedError("TensorFlow client not yet implemented")\n    elif metadata.provider.value == "tensorrt":\n        # TODO: Implement TensorRT client\n        raise NotImplementedError("TensorRT client not yet implemented")\n    else:\n        # Default to PyTorch for custom models\n        return PyTorchClient(metadata)